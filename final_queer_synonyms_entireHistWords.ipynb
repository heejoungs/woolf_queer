{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Positivity of the Words Closest to \"Queer\" for Each Decade from HistWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Queer\"'s vector is missing from the 1800s to the 1840s. For this reason, I was able to extract words closest to \"queer\" from the 1950s and onwards. Please see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1800s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1800 = dict()\n",
    "\n",
    "hist_vocab_1800 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1800-vocab.pkl', 'rb'))\n",
    "hist_vectors_1800 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1800-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1800)):\n",
    "    hist_embeds_1800[hist_vocab_1800[i]] = hist_vectors_1800[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1800 = hist_embeds_1800['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_queer_1800 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1810s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1810 = dict()\n",
    "\n",
    "hist_vocab_1810 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1810-vocab.pkl', 'rb'))\n",
    "hist_vectors_1810 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1810-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1810)):\n",
    "    hist_embeds_1810[hist_vocab_1810[i]] = hist_vectors_1810[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1810 = hist_embeds_1810['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_queer_1810 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1820s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1820 = dict()\n",
    "\n",
    "hist_vocab_1820 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1820-vocab.pkl', 'rb'))\n",
    "hist_vectors_1820 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1820-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1820)):\n",
    "    hist_embeds_1820[hist_vocab_1820[i]] = hist_vectors_1820[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1820 = hist_embeds_1820['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_queer_1820 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1830s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1830 = dict()\n",
    "\n",
    "hist_vocab_1830 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1830-vocab.pkl', 'rb'))\n",
    "hist_vectors_1830 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1830-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1830)):\n",
    "    hist_embeds_1830[hist_vocab_1830[i]] = hist_vectors_1830[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1830 = hist_embeds_1830['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_queer_1830 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1840s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1840 = dict()\n",
    "\n",
    "hist_vocab_1840 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1840-vocab.pkl', 'rb'))\n",
    "hist_vectors_1840 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1840-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1840)):\n",
    "    hist_embeds_1840[hist_vocab_1840[i]] = hist_vectors_1840[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1840 = hist_embeds_1840['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_queer_1840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1850s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1850 = dict()\n",
    "\n",
    "hist_vocab_1850 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1850-vocab.pkl', 'rb'))\n",
    "hist_vectors_1850 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1850-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1850)):\n",
    "    hist_embeds_1850[hist_vocab_1850[i]] = hist_vectors_1850[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1850 = hist_embeds_1850['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1850 = dict()\n",
    "for v in hist_vocab_1850:\n",
    "    vectordict_1850[v] = hist_embeds_1850[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1850 = pd.DataFrame.from_dict(vectordict_1850, orient = 'index')\n",
    "vectordf_1850[\"sum\"] = vectordf_1850.sum(axis=1)\n",
    "valuezero_1850 = vectordf_1850['sum'].isin([0.0])\n",
    "vectordf_1850[~valuezero_1850]\n",
    "vectordf_no_zeros_1850 = vectordf_1850[~valuezero_1850]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1850['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1850):\n",
    "    \n",
    "    vectordict_1850 = dict()\n",
    "    for v in hist_vocab_1850:\n",
    "        vectordict_1850[v] = hist_embeds_1850[v]\n",
    "        \n",
    "    vectordf_1850 = pd.DataFrame.from_dict(vectordict_1850, orient = 'index')\n",
    "    vectordf_1850[\"sum\"] = vectordf_1850.sum(axis=1)\n",
    "    valuezero_1850 = vectordf_1850['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1850 = vectordf_1850[~valuezero_1850]\n",
    "    del vectordf_no_zeros_1850['sum']\n",
    "    \n",
    "    vectorlengths_1850 = np.linalg.norm(vectordf_no_zeros_1850, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1850 = vectordf_no_zeros_1850.divide(vectorlengths_1850, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1850 = list(vectordf_no_zeros_1850.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1850 = pca.fit_transform(vectordf_no_zeros_1850)                      # perform PCA\n",
    "    plotframe_1850 = pd.DataFrame({'x': components_1850[ : , 0], 'y': components_1850[ : , 1]}, index = hist_vocab_no_zeros_1850)\n",
    "    \n",
    "    return plotframe_1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>-0.025114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.057229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x -0.025114\n",
       "y  0.057229"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1850 = vocab2pca(hist_vocab_1850)\n",
    "transposed_1850 = plotframe_1850.transpose()\n",
    "transposed_1850[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1850 = transposed_1850.iloc[0]['queer']\n",
    "queer_y_1850 = transposed_1850.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1850[\"distance_from_queer\"] = (((plotframe_1850.x-queer_x_1850)**2+(plotframe_1850.y-queer_y_1850)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1850 = plotframe_1850.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1850.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1850.index)[0:100]\n",
    "queersimilar_1850 = list(distance_sorted_1850.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1850 = [w for w in queersimilar_1850 if w in negative_words]\n",
    "positivelist_1850 = [w for w in queersimilar_1850 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1850  = vectordf_no_zeros_1850[vectordf_no_zeros_1850.index.isin(negativelist_1850)]\n",
    "positive_df_1850  = vectordf_no_zeros_1850[vectordf_no_zeros_1850.index.isin(positivelist_1850)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fair', 'succeeded', 'safely']\n",
      "['despair', 'anger', 'agony', 'bitter', 'exhausted', 'indignation', 'perish', 'queer']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1850 = positive_df_1850.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1850)\n",
    "negative_list_1850 = negative_df_1850.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.4603573062353934\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1850 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1850[ex]    # get its vector\n",
    "        vectorlength_1850 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1850.append(vec / vectorlength_1850)              # and save it in the list\n",
    "        \n",
    "    thesum_1850 = np.sum(vectors_1850, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1850 = np.linalg.norm(thesum_1850, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1850 / vectorlength_1850\n",
    "\n",
    "\n",
    "pos_vector_1850 = get_vector(positive_list_1850)\n",
    "neg_vector_1850 = get_vector(negative_list_1850)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1850 = cosine(vec_queer_1850, pos_vector_1850)\n",
    "neg_dist_1850 = cosine(vec_queer_1850, neg_vector_1850)\n",
    "\n",
    "    \n",
    "net_positive_queer_1850 = neg_dist_1850 - pos_dist_1850\n",
    "    \n",
    "print(net_positive_queer_1850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1860s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1860 = dict()\n",
    "\n",
    "hist_vocab_1860 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1860-vocab.pkl', 'rb'))\n",
    "hist_vectors_1860 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1860-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1860)):\n",
    "    hist_embeds_1860[hist_vocab_1860[i]] = hist_vectors_1860[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1860 = hist_embeds_1860['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1860 = dict()\n",
    "for v in hist_vocab_1860:\n",
    "    vectordict_1860[v] = hist_embeds_1860[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1860 = pd.DataFrame.from_dict(vectordict_1860, orient = 'index')\n",
    "vectordf_1860[\"sum\"] = vectordf_1860.sum(axis=1)\n",
    "valuezero_1860 = vectordf_1860['sum'].isin([0.0])\n",
    "vectordf_1860[~valuezero_1860]\n",
    "vectordf_no_zeros_1860 = vectordf_1860[~valuezero_1860]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1860['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1860):\n",
    "    \n",
    "    vectordict_1860 = dict()\n",
    "    for v in hist_vocab_1860:\n",
    "        vectordict_1860[v] = hist_embeds_1860[v]\n",
    "        \n",
    "    vectordf_1860 = pd.DataFrame.from_dict(vectordict_1860, orient = 'index')\n",
    "    vectordf_1860[\"sum\"] = vectordf_1860.sum(axis=1)\n",
    "    valuezero_1860 = vectordf_1860['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1860 = vectordf_1860[~valuezero_1860]\n",
    "    del vectordf_no_zeros_1860['sum']\n",
    "    \n",
    "    vectorlengths_1860 = np.linalg.norm(vectordf_no_zeros_1860, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1860 = vectordf_no_zeros_1860.divide(vectorlengths_1860, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1860 = list(vectordf_no_zeros_1860.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1860 = pca.fit_transform(vectordf_no_zeros_1860)                      # perform PCA\n",
    "    plotframe_1860 = pd.DataFrame({'x': components_1860[ : , 0], 'y': components_1860[ : , 1]}, index = hist_vocab_no_zeros_1860)\n",
    "    \n",
    "    return plotframe_1860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.163671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.043138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.163671\n",
       "y -0.043138"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1860 = vocab2pca(hist_vocab_1860)\n",
    "transposed_1860 = plotframe_1860.transpose()\n",
    "transposed_1860[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1860 = transposed_1860.iloc[0]['queer']\n",
    "queer_y_1860 = transposed_1860.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1860[\"distance_from_queer\"] = (((plotframe_1860.x-queer_x_1860)**2+(plotframe_1860.y-queer_y_1860)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1860 = plotframe_1860.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1860.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1860.index)[0:100]\n",
    "queersimilar_1860 = list(distance_sorted_1860.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1860 = [w for w in queersimilar_1860 if w in negative_words]\n",
    "positivelist_1860 = [w for w in queersimilar_1860 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1860  = vectordf_no_zeros_1860[vectordf_no_zeros_1860.index.isin(negativelist_1860)]\n",
    "positive_df_1860  = vectordf_no_zeros_1860[vectordf_no_zeros_1860.index.isin(positivelist_1860)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fresh', 'steady', 'smart']\n",
      "['prison', 'rage', 'lonely', 'queer', 'hell']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1860 = positive_df_1860.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1860)\n",
    "negative_list_1860 = negative_df_1860.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07281647113300116\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1860 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1860[ex]    # get its vector\n",
    "        vectorlength_1860 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1860.append(vec / vectorlength_1860)              # and save it in the list\n",
    "        \n",
    "    thesum_1860 = np.sum(vectors_1860, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1860 = np.linalg.norm(thesum_1860, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1860 / vectorlength_1860\n",
    "\n",
    "\n",
    "pos_vector_1860 = get_vector(positive_list_1860)\n",
    "neg_vector_1860 = get_vector(negative_list_1860)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1860 = cosine(vec_queer_1860, pos_vector_1860)\n",
    "neg_dist_1860 = cosine(vec_queer_1860, neg_vector_1860)\n",
    "\n",
    "    \n",
    "net_positive_queer_1860 = neg_dist_1860 - pos_dist_1860\n",
    "    \n",
    "print(net_positive_queer_1860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1870s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1870 = dict()\n",
    "\n",
    "hist_vocab_1870 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1870-vocab.pkl', 'rb'))\n",
    "hist_vectors_1870 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1870-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1870)):\n",
    "    hist_embeds_1870[hist_vocab_1870[i]] = hist_vectors_1870[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1870 = hist_embeds_1870['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1870 = dict()\n",
    "for v in hist_vocab_1870:\n",
    "    vectordict_1870[v] = hist_embeds_1870[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1870 = pd.DataFrame.from_dict(vectordict_1870, orient = 'index')\n",
    "vectordf_1870[\"sum\"] = vectordf_1870.sum(axis=1)\n",
    "valuezero_1870 = vectordf_1870['sum'].isin([0.0])\n",
    "vectordf_1870[~valuezero_1870]\n",
    "vectordf_no_zeros_1870 = vectordf_1870[~valuezero_1870]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1870['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1870):\n",
    "    \n",
    "    vectordict_1870 = dict()\n",
    "    for v in hist_vocab_1870:\n",
    "        vectordict_1870[v] = hist_embeds_1870[v]\n",
    "        \n",
    "    vectordf_1870 = pd.DataFrame.from_dict(vectordict_1870, orient = 'index')\n",
    "    vectordf_1870[\"sum\"] = vectordf_1870.sum(axis=1)\n",
    "    valuezero_1870 = vectordf_1870['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1870 = vectordf_1870[~valuezero_1870]\n",
    "    del vectordf_no_zeros_1870['sum']\n",
    "    \n",
    "    vectorlengths_1870 = np.linalg.norm(vectordf_no_zeros_1870, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1870 = vectordf_no_zeros_1870.divide(vectorlengths_1870, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1870 = list(vectordf_no_zeros_1870.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1870 = pca.fit_transform(vectordf_no_zeros_1870)                      # perform PCA\n",
    "    plotframe_1870 = pd.DataFrame({'x': components_1870[ : , 0], 'y': components_1870[ : , 1]}, index = hist_vocab_no_zeros_1870)\n",
    "    \n",
    "    return plotframe_1870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>-0.013053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.026740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x -0.013053\n",
       "y -0.026740"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1870 = vocab2pca(hist_vocab_1870)\n",
    "transposed_1870 = plotframe_1870.transpose()\n",
    "transposed_1870[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1870 = transposed_1870.iloc[0]['queer']\n",
    "queer_y_1870 = transposed_1870.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1870[\"distance_from_queer\"] = (((plotframe_1870.x-queer_x_1870)**2+(plotframe_1870.y-queer_y_1870)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1870 = plotframe_1870.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1870.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1870.index)[0:100]\n",
    "queersimilar_1870 = list(distance_sorted_1870.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1870 = [w for w in queersimilar_1870 if w in negative_words]\n",
    "positivelist_1870 = [w for w in queersimilar_1870 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1870  = vectordf_no_zeros_1870[vectordf_no_zeros_1870.index.isin(negativelist_1870)]\n",
    "positive_df_1870  = vectordf_no_zeros_1870[vectordf_no_zeros_1870.index.isin(positivelist_1870)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delight', 'passion', 'gentle', 'lovely', 'cheerful']\n",
      "['slow', 'shock', 'agony', 'ugly', 'scorn', 'helpless', 'queer', 'stolen', 'lapse', 'bewildered', 'perish']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1870 = positive_df_1870.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1870)\n",
    "negative_list_1870 = negative_df_1870.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15109691124396774\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1870 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1870[ex]    # get its vector\n",
    "        vectorlength_1870 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1870.append(vec / vectorlength_1870)              # and save it in the list\n",
    "        \n",
    "    thesum_1870 = np.sum(vectors_1870, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1870 = np.linalg.norm(thesum_1870, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1870 / vectorlength_1870\n",
    "\n",
    "\n",
    "pos_vector_1870 = get_vector(positive_list_1870)\n",
    "neg_vector_1870 = get_vector(negative_list_1870)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1870 = cosine(vec_queer_1870, pos_vector_1870)\n",
    "neg_dist_1870 = cosine(vec_queer_1870, neg_vector_1870)\n",
    "\n",
    "    \n",
    "net_positive_queer_1870 = neg_dist_1870 - pos_dist_1870\n",
    "    \n",
    "print(net_positive_queer_1870)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1880s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1880 = dict()\n",
    "\n",
    "hist_vocab_1880 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1880-vocab.pkl', 'rb'))\n",
    "hist_vectors_1880 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1880-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1880)):\n",
    "    hist_embeds_1880[hist_vocab_1880[i]] = hist_vectors_1880[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1880 = hist_embeds_1880['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1880 = dict()\n",
    "for v in hist_vocab_1880:\n",
    "    vectordict_1880[v] = hist_embeds_1880[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1880 = pd.DataFrame.from_dict(vectordict_1880, orient = 'index')\n",
    "vectordf_1880[\"sum\"] = vectordf_1880.sum(axis=1)\n",
    "valuezero_1880 = vectordf_1880['sum'].isin([0.0])\n",
    "vectordf_1880[~valuezero_1880]\n",
    "vectordf_no_zeros_1880 = vectordf_1880[~valuezero_1880]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1880['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1880):\n",
    "    \n",
    "    vectordict_1880 = dict()\n",
    "    for v in hist_vocab_1880:\n",
    "        vectordict_1880[v] = hist_embeds_1880[v]\n",
    "        \n",
    "    vectordf_1880 = pd.DataFrame.from_dict(vectordict_1880, orient = 'index')\n",
    "    vectordf_1880[\"sum\"] = vectordf_1880.sum(axis=1)\n",
    "    valuezero_1880 = vectordf_1880['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1880 = vectordf_1880[~valuezero_1880]\n",
    "    del vectordf_no_zeros_1880['sum']\n",
    "    \n",
    "    vectorlengths_1880 = np.linalg.norm(vectordf_no_zeros_1880, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1880 = vectordf_no_zeros_1880.divide(vectorlengths_1880, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1880 = list(vectordf_no_zeros_1880.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1880 = pca.fit_transform(vectordf_no_zeros_1880)                      # perform PCA\n",
    "    plotframe_1880 = pd.DataFrame({'x': components_1880[ : , 0], 'y': components_1880[ : , 1]}, index = hist_vocab_no_zeros_1880)\n",
    "    \n",
    "    return plotframe_1880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.089068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.045503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.089068\n",
       "y  0.045503"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1880 = vocab2pca(hist_vocab_1880)\n",
    "transposed_1880 = plotframe_1880.transpose()\n",
    "transposed_1880[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1880 = transposed_1880.iloc[0]['queer']\n",
    "queer_y_1880 = transposed_1880.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1880[\"distance_from_queer\"] = (((plotframe_1880.x-queer_x_1880)**2+(plotframe_1880.y-queer_y_1880)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1880 = plotframe_1880.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1880.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1880.index)[0:100]\n",
    "queersimilar_1880 = list(distance_sorted_1880.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1880 = [w for w in queersimilar_1880 if w in negative_words]\n",
    "positivelist_1880 = [w for w in queersimilar_1880 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1880  = vectordf_no_zeros_1880[vectordf_no_zeros_1880.index.isin(negativelist_1880)]\n",
    "positive_df_1880  = vectordf_no_zeros_1880[vectordf_no_zeros_1880.index.isin(positivelist_1880)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worked', 'saint', 'leads', 'jolly']\n",
      "['wound', 'waste', 'stolen', 'queer', 'struggled', 'dies']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1880 = positive_df_1880.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1880)\n",
    "negative_list_1880 = negative_df_1880.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1970525110263902\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1880 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1880[ex]    # get its vector\n",
    "        vectorlength_1880 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1880.append(vec / vectorlength_1880)              # and save it in the list\n",
    "        \n",
    "    thesum_1880 = np.sum(vectors_1880, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1880 = np.linalg.norm(thesum_1880, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1880 / vectorlength_1880\n",
    "\n",
    "\n",
    "pos_vector_1880 = get_vector(positive_list_1880)\n",
    "neg_vector_1880 = get_vector(negative_list_1880)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1880 = cosine(vec_queer_1880, pos_vector_1880)\n",
    "neg_dist_1880 = cosine(vec_queer_1880, neg_vector_1880)\n",
    "\n",
    "    \n",
    "net_positive_queer_1880 = neg_dist_1880 - pos_dist_1880\n",
    "    \n",
    "print(net_positive_queer_1880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1890s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1890 = dict()\n",
    "\n",
    "hist_vocab_1890 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1890-vocab.pkl', 'rb'))\n",
    "hist_vectors_1890 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1890-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1890)):\n",
    "    hist_embeds_1890[hist_vocab_1890[i]] = hist_vectors_1890[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1890 = hist_embeds_1890['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1890 = dict()\n",
    "for v in hist_vocab_1890:\n",
    "    vectordict_1890[v] = hist_embeds_1890[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1890 = pd.DataFrame.from_dict(vectordict_1890, orient = 'index')\n",
    "vectordf_1890[\"sum\"] = vectordf_1890.sum(axis=1)\n",
    "valuezero_1890 = vectordf_1890['sum'].isin([0.0])\n",
    "vectordf_1890[~valuezero_1890]\n",
    "vectordf_no_zeros_1890 = vectordf_1890[~valuezero_1890]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1890['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1890):\n",
    "    \n",
    "    vectordict_1890 = dict()\n",
    "    for v in hist_vocab_1890:\n",
    "        vectordict_1890[v] = hist_embeds_1890[v]\n",
    "        \n",
    "    vectordf_1890 = pd.DataFrame.from_dict(vectordict_1890, orient = 'index')\n",
    "    vectordf_1890[\"sum\"] = vectordf_1890.sum(axis=1)\n",
    "    valuezero_1890 = vectordf_1890['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1890 = vectordf_1890[~valuezero_1890]\n",
    "    del vectordf_no_zeros_1890['sum']\n",
    "    \n",
    "    vectorlengths_1890 = np.linalg.norm(vectordf_no_zeros_1890, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1890 = vectordf_no_zeros_1890.divide(vectorlengths_1890, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1890 = list(vectordf_no_zeros_1890.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1890 = pca.fit_transform(vectordf_no_zeros_1890)                      # perform PCA\n",
    "    plotframe_1890 = pd.DataFrame({'x': components_1890[ : , 0], 'y': components_1890[ : , 1]}, index = hist_vocab_no_zeros_1890)\n",
    "    \n",
    "    return plotframe_1890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.068041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.025008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.068041\n",
       "y  0.025008"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1890 = vocab2pca(hist_vocab_1890)\n",
    "transposed_1890 = plotframe_1890.transpose()\n",
    "transposed_1890[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1890 = transposed_1890.iloc[0]['queer']\n",
    "queer_y_1890 = transposed_1890.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1890[\"distance_from_queer\"] = (((plotframe_1890.x-queer_x_1890)**2+(plotframe_1890.y-queer_y_1890)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1890 = plotframe_1890.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1890.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1890.index)[0:100]\n",
    "queersimilar_1890 = list(distance_sorted_1890.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1890 = [w for w in queersimilar_1890 if w in negative_words]\n",
    "positivelist_1890 = [w for w in queersimilar_1890 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1890  = vectordf_no_zeros_1890[vectordf_no_zeros_1890.index.isin(negativelist_1890)]\n",
    "positive_df_1890  = vectordf_no_zeros_1890[vectordf_no_zeros_1890.index.isin(positivelist_1890)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['silent', 'sweet', 'warm', 'mighty', 'leads']\n",
      "['lies', 'waste', 'fever', 'lonely', 'queer', 'cheap', 'slaves', 'punch', 'madman']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1890 = positive_df_1890.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1890)\n",
    "negative_list_1890 = negative_df_1890.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3919222493629885\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1890 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1890[ex]    # get its vector\n",
    "        vectorlength_1890 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1890.append(vec / vectorlength_1890)              # and save it in the list\n",
    "        \n",
    "    thesum_1890 = np.sum(vectors_1890, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1890 = np.linalg.norm(thesum_1890, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1890 / vectorlength_1890\n",
    "\n",
    "\n",
    "pos_vector_1890 = get_vector(positive_list_1890)\n",
    "neg_vector_1890 = get_vector(negative_list_1890)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1890 = cosine(vec_queer_1890, pos_vector_1890)\n",
    "neg_dist_1890 = cosine(vec_queer_1890, neg_vector_1890)\n",
    "\n",
    "    \n",
    "net_positive_queer_1890 = neg_dist_1890 - pos_dist_1890\n",
    "    \n",
    "print(net_positive_queer_1890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1900s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1900 = dict()\n",
    "\n",
    "hist_vocab_1900 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1900-vocab.pkl', 'rb'))\n",
    "hist_vectors_1900 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1900-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1900)):\n",
    "    hist_embeds_1900[hist_vocab_1900[i]] = hist_vectors_1900[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1900 = hist_embeds_1900['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1900 = dict()\n",
    "for v in hist_vocab_1900:\n",
    "    vectordict_1900[v] = hist_embeds_1900[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1900 = pd.DataFrame.from_dict(vectordict_1900, orient = 'index')\n",
    "vectordf_1900[\"sum\"] = vectordf_1900.sum(axis=1)\n",
    "valuezero_1900 = vectordf_1900['sum'].isin([0.0])\n",
    "vectordf_1900[~valuezero_1900]\n",
    "vectordf_no_zeros_1900 = vectordf_1900[~valuezero_1900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1900['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1900):\n",
    "    \n",
    "    vectordict_1900 = dict()\n",
    "    for v in hist_vocab_1900:\n",
    "        vectordict_1900[v] = hist_embeds_1900[v]\n",
    "        \n",
    "    vectordf_1900 = pd.DataFrame.from_dict(vectordict_1900, orient = 'index')\n",
    "    vectordf_1900[\"sum\"] = vectordf_1900.sum(axis=1)\n",
    "    valuezero_1900 = vectordf_1900['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1900 = vectordf_1900[~valuezero_1900]\n",
    "    del vectordf_no_zeros_1900['sum']\n",
    "    \n",
    "    vectorlengths_1900 = np.linalg.norm(vectordf_no_zeros_1900, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1900 = vectordf_no_zeros_1900.divide(vectorlengths_1900, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1900 = list(vectordf_no_zeros_1900.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1900 = pca.fit_transform(vectordf_no_zeros_1900)                      # perform PCA\n",
    "    plotframe_1900 = pd.DataFrame({'x': components_1900[ : , 0], 'y': components_1900[ : , 1]}, index = hist_vocab_no_zeros_1900)\n",
    "    \n",
    "    return plotframe_1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.026969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.024372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.026969\n",
       "y -0.024372"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1900 = vocab2pca(hist_vocab_1900)\n",
    "transposed_1900 = plotframe_1900.transpose()\n",
    "transposed_1900[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1900 = transposed_1900.iloc[0]['queer']\n",
    "queer_y_1900 = transposed_1900.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1900[\"distance_from_queer\"] = (((plotframe_1900.x-queer_x_1900)**2+(plotframe_1900.y-queer_y_1900)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1900 = plotframe_1900.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1900.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1900.index)[0:100]\n",
    "queersimilar_1900 = list(distance_sorted_1900.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1900 = [w for w in queersimilar_1900 if w in negative_words]\n",
    "positivelist_1900 = [w for w in queersimilar_1900 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1900  = vectordf_no_zeros_1900[vectordf_no_zeros_1900.index.isin(negativelist_1900)]\n",
    "positive_df_1900  = vectordf_no_zeros_1900[vectordf_no_zeros_1900.index.isin(positivelist_1900)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beautiful', 'lovely', 'nicely']\n",
      "['lies', 'fever', 'queer', 'wounds', 'furious', 'stab', 'scornfully']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1900 = positive_df_1900.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1900)\n",
    "negative_list_1900 = negative_df_1900.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.1985968466523007\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1900 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1900[ex]    # get its vector\n",
    "        vectorlength_1900 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1900.append(vec / vectorlength_1900)              # and save it in the list\n",
    "        \n",
    "    thesum_1900 = np.sum(vectors_1900, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1900 = np.linalg.norm(thesum_1900, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1900 / vectorlength_1900\n",
    "\n",
    "\n",
    "pos_vector_1900 = get_vector(positive_list_1900)\n",
    "neg_vector_1900 = get_vector(negative_list_1900)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1900 = cosine(vec_queer_1900, pos_vector_1900)\n",
    "neg_dist_1900 = cosine(vec_queer_1900, neg_vector_1900)\n",
    "\n",
    "    \n",
    "net_positive_queer_1900 = neg_dist_1900 - pos_dist_1900\n",
    "    \n",
    "print(net_positive_queer_1900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1910s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1910 = dict()\n",
    "\n",
    "hist_vocab_1910 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1910-vocab.pkl', 'rb'))\n",
    "hist_vectors_1910 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1910-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1910)):\n",
    "    hist_embeds_1910[hist_vocab_1910[i]] = hist_vectors_1910[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1910 = hist_embeds_1910['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1910 = dict()\n",
    "for v in hist_vocab_1910:\n",
    "    vectordict_1910[v] = hist_embeds_1910[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1910 = pd.DataFrame.from_dict(vectordict_1910, orient = 'index')\n",
    "vectordf_1910[\"sum\"] = vectordf_1910.sum(axis=1)\n",
    "valuezero_1910 = vectordf_1910['sum'].isin([0.0])\n",
    "vectordf_1910[~valuezero_1910]\n",
    "vectordf_no_zeros_1910 = vectordf_1910[~valuezero_1910]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1910['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1910):\n",
    "    \n",
    "    vectordict_1910 = dict()\n",
    "    for v in hist_vocab_1910:\n",
    "        vectordict_1910[v] = hist_embeds_1910[v]\n",
    "        \n",
    "    vectordf_1910 = pd.DataFrame.from_dict(vectordict_1910, orient = 'index')\n",
    "    vectordf_1910[\"sum\"] = vectordf_1910.sum(axis=1)\n",
    "    valuezero_1910 = vectordf_1910['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1910 = vectordf_1910[~valuezero_1910]\n",
    "    del vectordf_no_zeros_1910['sum']\n",
    "    \n",
    "    vectorlengths_1910 = np.linalg.norm(vectordf_no_zeros_1910, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1910 = vectordf_no_zeros_1910.divide(vectorlengths_1910, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1910 = list(vectordf_no_zeros_1910.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1910 = pca.fit_transform(vectordf_no_zeros_1910)                      # perform PCA\n",
    "    plotframe_1910 = pd.DataFrame({'x': components_1910[ : , 0], 'y': components_1910[ : , 1]}, index = hist_vocab_no_zeros_1910)\n",
    "    \n",
    "    return plotframe_1910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.044128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.174721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.044128\n",
       "y  0.174721"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1910 = vocab2pca(hist_vocab_1910)\n",
    "transposed_1910 = plotframe_1910.transpose()\n",
    "transposed_1910[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1910 = transposed_1910.iloc[0]['queer']\n",
    "queer_y_1910 = transposed_1910.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1910[\"distance_from_queer\"] = (((plotframe_1910.x-queer_x_1910)**2+(plotframe_1910.y-queer_y_1910)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1910 = plotframe_1910.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1910.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1910.index)[0:100]\n",
    "queersimilar_1910 = list(distance_sorted_1910.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1910 = [w for w in queersimilar_1910 if w in negative_words]\n",
    "positivelist_1910 = [w for w in queersimilar_1910 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1910  = vectordf_no_zeros_1910[vectordf_no_zeros_1910.index.isin(negativelist_1910)]\n",
    "positive_df_1910  = vectordf_no_zeros_1910[vectordf_no_zeros_1910.index.isin(positivelist_1910)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazement', 'smart', 'pleasantly', 'passionately', 'patiently', 'quicker', 'sweetly', 'respectfully']\n",
      "['break', 'anger', 'dying', 'drunk', 'queer', 'agony', 'mess', 'steal', 'dismay', 'bewildered', 'ache', 'sue', 'emphatically']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1910=positive_df_1910.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1910)\n",
    "negative_list_1910=negative_df_1910.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2861370412750823\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1910 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1910[ex]    # get its vector\n",
    "        vectorlength_1910 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1910.append(vec / vectorlength_1910)              # and save it in the list\n",
    "        \n",
    "    thesum_1910 = np.sum(vectors_1910, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1910 = np.linalg.norm(thesum_1910, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1910 / vectorlength_1910\n",
    "\n",
    "\n",
    "pos_vector_1910 = get_vector(positive_list_1910)\n",
    "neg_vector_1910 = get_vector(negative_list_1910)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1910 = cosine(vec_queer_1910, pos_vector_1910)\n",
    "neg_dist_1910 = cosine(vec_queer_1910, neg_vector_1910)\n",
    "\n",
    "    \n",
    "net_positive_queer_1910 = neg_dist_1910 - pos_dist_1910\n",
    "    \n",
    "print(net_positive_queer_1910)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1920s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1920 = dict()\n",
    "\n",
    "hist_vocab_1920 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1920-vocab.pkl', 'rb'))\n",
    "hist_vectors_1920 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1920-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1920)):\n",
    "    hist_embeds_1920[hist_vocab_1920[i]] = hist_vectors_1920[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1920 = hist_embeds_1920['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1920 = dict()\n",
    "for v in hist_vocab_1920:\n",
    "    vectordict_1920[v] = hist_embeds_1920[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1920 = pd.DataFrame.from_dict(vectordict_1920, orient = 'index')\n",
    "vectordf_1920[\"sum\"] = vectordf_1920.sum(axis=1)\n",
    "valuezero_1920 = vectordf_1920['sum'].isin([0.0])\n",
    "vectordf_1920[~valuezero_1920]\n",
    "vectordf_no_zeros_1920 = vectordf_1920[~valuezero_1920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1920['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1920):\n",
    "    \n",
    "    vectordict_1920 = dict()\n",
    "    for v in hist_vocab_1920:\n",
    "        vectordict_1920[v] = hist_embeds_1920[v]\n",
    "        \n",
    "    vectordf_1920 = pd.DataFrame.from_dict(vectordict_1920, orient = 'index')\n",
    "    vectordf_1920[\"sum\"] = vectordf_1920.sum(axis=1)\n",
    "    valuezero_1920 = vectordf_1920['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1920 = vectordf_1920[~valuezero_1920]\n",
    "    del vectordf_no_zeros_1920['sum']\n",
    "    \n",
    "    vectorlengths_1920 = np.linalg.norm(vectordf_no_zeros_1920, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1920 = vectordf_no_zeros_1920.divide(vectorlengths_1920, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1920 = list(vectordf_no_zeros_1920.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1920 = pca.fit_transform(vectordf_no_zeros_1920)                      # perform PCA\n",
    "    plotframe_1920 = pd.DataFrame({'x': components_1920[ : , 0], 'y': components_1920[ : , 1]}, index = hist_vocab_no_zeros_1920)\n",
    "    \n",
    "    return plotframe_1920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.028866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.039556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.028866\n",
       "y -0.039556"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1920 = vocab2pca(hist_vocab_1920)\n",
    "transposed_1920 = plotframe_1920.transpose()\n",
    "transposed_1920[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1920 = transposed_1920.iloc[0]['queer']\n",
    "queer_y_1920 = transposed_1920.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1920[\"distance_from_queer\"] = (((plotframe_1920.x-queer_x_1920)**2+(plotframe_1920.y-queer_y_1920)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1920 = plotframe_1920.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1920.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1920.index)[0:100]\n",
    "queersimilar_1920 = list(distance_sorted_1920.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1920 = [w for w in queersimilar_1920 if w in negative_words]\n",
    "positivelist_1920 = [w for w in queersimilar_1920 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1920  = vectordf_no_zeros_1920[vectordf_no_zeros_1920.index.isin(negativelist_1920)]\n",
    "positive_df_1920  = vectordf_no_zeros_1920[vectordf_no_zeros_1920.index.isin(positivelist_1920)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quiet', 'sweet', 'treasure', 'leads', 'favorite', 'stunned', 'sweetly']\n",
      "['blind', 'waste', 'queer', 'fever', 'burden', 'exhausted', 'dumb', 'helpless', 'fright', 'ache', 'drunken', 'drowning', 'oddly', 'stab']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1920 = positive_df_1920.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1920)\n",
    "negative_list_1920 = negative_df_1920.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.20661738641908522\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1920 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1920[ex]    # get its vector\n",
    "        vectorlength_1920 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1920.append(vec / vectorlength_1920)              # and save it in the list\n",
    "        \n",
    "    thesum_1920 = np.sum(vectors_1920, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1920 = np.linalg.norm(thesum_1920, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1920 / vectorlength_1920\n",
    "\n",
    "\n",
    "pos_vector_1920 = get_vector(positive_list_1920)\n",
    "neg_vector_1920 = get_vector(negative_list_1920)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1920 = cosine(vec_queer_1920, pos_vector_1920)\n",
    "neg_dist_1920 = cosine(vec_queer_1920, neg_vector_1920)\n",
    "\n",
    "    \n",
    "net_positive_queer_1920 = neg_dist_1920 - pos_dist_1920\n",
    "    \n",
    "print(net_positive_queer_1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1930s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1930 = dict()\n",
    "\n",
    "hist_vocab_1930 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1930-vocab.pkl', 'rb'))\n",
    "hist_vectors_1930 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1930-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1930)):\n",
    "    hist_embeds_1930[hist_vocab_1930[i]] = hist_vectors_1930[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1930 = hist_embeds_1930['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1930 = dict()\n",
    "for v in hist_vocab_1930:\n",
    "    vectordict_1930[v] = hist_embeds_1930[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1930 = pd.DataFrame.from_dict(vectordict_1930, orient = 'index')\n",
    "vectordf_1930[\"sum\"] = vectordf_1930.sum(axis=1)\n",
    "valuezero_1930 = vectordf_1930['sum'].isin([0.0])\n",
    "vectordf_1930[~valuezero_1930]\n",
    "vectordf_no_zeros_1930 = vectordf_1930[~valuezero_1930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1930['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1930):\n",
    "    \n",
    "    vectordict_1930 = dict()\n",
    "    for v in hist_vocab_1930:\n",
    "        vectordict_1930[v] = hist_embeds_1930[v]\n",
    "        \n",
    "    vectordf_1930 = pd.DataFrame.from_dict(vectordict_1930, orient = 'index')\n",
    "    vectordf_1930[\"sum\"] = vectordf_1930.sum(axis=1)\n",
    "    valuezero_1930 = vectordf_1930['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1930 = vectordf_1930[~valuezero_1930]\n",
    "    del vectordf_no_zeros_1930['sum']\n",
    "    \n",
    "    vectorlengths_1930 = np.linalg.norm(vectordf_no_zeros_1930, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1930 = vectordf_no_zeros_1930.divide(vectorlengths_1930, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1930 = list(vectordf_no_zeros_1930.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1930 = pca.fit_transform(vectordf_no_zeros_1930)                      # perform PCA\n",
    "    plotframe_1930 = pd.DataFrame({'x': components_1930[ : , 0], 'y': components_1930[ : , 1]}, index = hist_vocab_no_zeros_1930)\n",
    "    \n",
    "    return plotframe_1930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.032025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.024968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.032025\n",
       "y  0.024968"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1930 = vocab2pca(hist_vocab_1930)\n",
    "transposed_1930 = plotframe_1930.transpose()\n",
    "transposed_1930[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1930 = transposed_1930.iloc[0]['queer']\n",
    "queer_y_1930 = transposed_1930.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1930[\"distance_from_queer\"] = (((plotframe_1930.x-queer_x_1930)**2+(plotframe_1930.y-queer_y_1930)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1930 = plotframe_1930.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1930.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1930.index)[0:100]\n",
    "queersimilar_1930 = list(distance_sorted_1930.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1930 = [w for w in queersimilar_1930 if w in negative_words]\n",
    "positivelist_1930 = [w for w in queersimilar_1930 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1930  = vectordf_no_zeros_1930[vectordf_no_zeros_1930.index.isin(negativelist_1930)]\n",
    "positive_df_1930  = vectordf_no_zeros_1930[vectordf_no_zeros_1930.index.isin(positivelist_1930)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clear', 'quiet', 'lead', 'pleasantly', 'stunned']\n",
      "['waste', 'queer', 'weary', 'cheap', 'cursed', 'mock', 'unexpectedly', 'sly', 'crush', 'burns', 'rot']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1930=positive_df_1930.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1930)\n",
    "negative_list_1930=negative_df_1930.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.405334859182119\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1930 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1930[ex]    # get its vector\n",
    "        vectorlength_1930 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1930.append(vec / vectorlength_1930)              # and save it in the list\n",
    "        \n",
    "    thesum_1930 = np.sum(vectors_1930, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1930 = np.linalg.norm(thesum_1930, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1930 / vectorlength_1930\n",
    "\n",
    "\n",
    "pos_vector_1930 = get_vector(positive_list_1930)\n",
    "neg_vector_1930 = get_vector(negative_list_1930)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1930 = cosine(vec_queer_1930, pos_vector_1930)\n",
    "neg_dist_1930 = cosine(vec_queer_1930, neg_vector_1930)\n",
    "\n",
    "    \n",
    "net_positive_queer_1930 = neg_dist_1930 - pos_dist_1930\n",
    "    \n",
    "print(net_positive_queer_1930)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1940s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1940 = dict()\n",
    "\n",
    "hist_vocab_1940 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1940-vocab.pkl', 'rb'))\n",
    "hist_vectors_1940 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1940-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1940)):\n",
    "    hist_embeds_1940[hist_vocab_1940[i]] = hist_vectors_1940[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1940 = hist_embeds_1940['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1940 = dict()\n",
    "for v in hist_vocab_1940:\n",
    "    vectordict_1940[v] = hist_embeds_1940[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1940 = pd.DataFrame.from_dict(vectordict_1940, orient = 'index')\n",
    "vectordf_1940[\"sum\"] = vectordf_1940.sum(axis=1)\n",
    "valuezero_1940 = vectordf_1940['sum'].isin([0.0])\n",
    "vectordf_1940[~valuezero_1940]\n",
    "vectordf_no_zeros_1940 = vectordf_1940[~valuezero_1940]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1940['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1940):\n",
    "    \n",
    "    vectordict_1940 = dict()\n",
    "    for v in hist_vocab_1940:\n",
    "        vectordict_1940[v] = hist_embeds_1940[v]\n",
    "        \n",
    "    vectordf_1940 = pd.DataFrame.from_dict(vectordict_1940, orient = 'index')\n",
    "    vectordf_1940[\"sum\"] = vectordf_1940.sum(axis=1)\n",
    "    valuezero_1940 = vectordf_1940['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1940 = vectordf_1940[~valuezero_1940]\n",
    "    del vectordf_no_zeros_1940['sum']\n",
    "    \n",
    "    vectorlengths_1940 = np.linalg.norm(vectordf_no_zeros_1940, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1940 = vectordf_no_zeros_1940.divide(vectorlengths_1940, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1940 = list(vectordf_no_zeros_1940.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1940 = pca.fit_transform(vectordf_no_zeros_1940)                      # perform PCA\n",
    "    plotframe_1940 = pd.DataFrame({'x': components_1940[ : , 0], 'y': components_1940[ : , 1]}, index = hist_vocab_no_zeros_1940)\n",
    "    \n",
    "    return plotframe_1940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.008755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.018017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.008755\n",
       "y  0.018017"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1940 = vocab2pca(hist_vocab_1940)\n",
    "transposed_1940 = plotframe_1940.transpose()\n",
    "transposed_1940[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1940 = transposed_1940.iloc[0]['queer']\n",
    "queer_y_1940 = transposed_1940.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1940[\"distance_from_queer\"] = (((plotframe_1940.x-queer_x_1940)**2+(plotframe_1940.y-queer_y_1940)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1940 = plotframe_1940.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1940.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1940.index)[0:100]\n",
    "queersimilar_1940 = list(distance_sorted_1940.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1940 = [w for w in queersimilar_1990 if w in negative_words]\n",
    "positivelist_1940 = [w for w in queersimilar_1990 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1940  = vectordf_no_zeros_1940[vectordf_no_zeros_1940.index.isin(negativelist_1940)]\n",
    "positive_df_1940  = vectordf_no_zeros_1940[vectordf_no_zeros_1940.index.isin(positivelist_1940)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheer', 'warmly', 'passionately', 'terrific', 'quicker', 'triumphantly']\n",
      "['queer', 'sadly', 'gravely', 'awkward', 'curse', 'coldly', 'pinch', 'abrupt', 'lame']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1940=positive_df_1940.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1940)\n",
    "negative_list_1940=negative_df_1940.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.33851549431590167\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1940 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1940[ex]    # get its vector\n",
    "        vectorlength_1940 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1940.append(vec / vectorlength_1940)              # and save it in the list\n",
    "        \n",
    "    thesum_1940 = np.sum(vectors_1940, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1940 = np.linalg.norm(thesum_1940, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1940 / vectorlength_1940\n",
    "\n",
    "\n",
    "pos_vector_1940 = get_vector(positive_list_1940)\n",
    "neg_vector_1940 = get_vector(negative_list_1940)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1940 = cosine(vec_queer_1940, pos_vector_1940)\n",
    "neg_dist_1940 = cosine(vec_queer_1940, neg_vector_1940)\n",
    "\n",
    "    \n",
    "net_positive_queer_1940 = neg_dist_1940 - pos_dist_1940\n",
    "    \n",
    "print(net_positive_queer_1940)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1950s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1950 = dict()\n",
    "\n",
    "hist_vocab_1950 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1950-vocab.pkl', 'rb'))\n",
    "hist_vectors_1950 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1950-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1950)):\n",
    "    hist_embeds_1950[hist_vocab_1950[i]] = hist_vectors_1950[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1950 = hist_embeds_1950['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1950 = dict()\n",
    "for v in hist_vocab_1950:\n",
    "    vectordict_1950[v] = hist_embeds_1950[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1950 = pd.DataFrame.from_dict(vectordict_1950, orient = 'index')\n",
    "vectordf_1950[\"sum\"] = vectordf_1950.sum(axis=1)\n",
    "valuezero_1950 = vectordf_1950['sum'].isin([0.0])\n",
    "vectordf_1950[~valuezero_1950]\n",
    "vectordf_no_zeros_1950 = vectordf_1950[~valuezero_1950]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1950['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1950):\n",
    "    \n",
    "    vectordict_1950 = dict()\n",
    "    for v in hist_vocab_1950:\n",
    "        vectordict_1950[v] = hist_embeds_1950[v]\n",
    "        \n",
    "    vectordf_1950 = pd.DataFrame.from_dict(vectordict_1950, orient = 'index')\n",
    "    vectordf_1950[\"sum\"] = vectordf_1950.sum(axis=1)\n",
    "    valuezero_1950 = vectordf_1950['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1950 = vectordf_1950[~valuezero_1950]\n",
    "    del vectordf_no_zeros_1950['sum']\n",
    "    \n",
    "    vectorlengths_1950 = np.linalg.norm(vectordf_no_zeros_1950, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1950 = vectordf_no_zeros_1950.divide(vectorlengths_1950, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1950 = list(vectordf_no_zeros_1950.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1950 = pca.fit_transform(vectordf_no_zeros_1950)                      # perform PCA\n",
    "    plotframe_1950 = pd.DataFrame({'x': components_1950[ : , 0], 'y': components_1950[ : , 1]}, index = hist_vocab_no_zeros_1950)\n",
    "    \n",
    "    return plotframe_1950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.011038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.113365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.011038\n",
       "y -0.113365"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1950 = vocab2pca(hist_vocab_1950)\n",
    "transposed_1950 = plotframe_1950.transpose()\n",
    "transposed_1950[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1950 = transposed_1950.iloc[0]['queer']\n",
    "queer_y_1950 = transposed_1950.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1950[\"distance_from_queer\"] = (((plotframe_1950.x-queer_x_1950)**2+(plotframe_1950.y-queer_y_1950)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1950 = plotframe_1950.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1950.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1950.index)[0:100]\n",
    "queersimilar_1950 = list(distance_sorted_1950.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1950 = [w for w in queersimilar_1950 if w in negative_words]\n",
    "positivelist_1950 = [w for w in queersimilar_1950 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1950  = vectordf_no_zeros_1950[vectordf_no_zeros_1950.index.isin(negativelist_1950)]\n",
    "positive_df_1950  = vectordf_no_zeros_1950[vectordf_no_zeros_1950.index.isin(positivelist_1950)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smile', 'striking', 'magic', 'magnificent', 'tenderly', 'peaceful', 'stunned', 'brighter', 'sweetly', 'refreshed']\n",
      "['bore', 'queer', 'exhausted', 'ugly', 'panic', 'foul', 'wounds', 'sly', 'crush', 'oddly', 'lull', 'fiend', 'contemptuously', 'boring', 'damaged', 'lowly']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1950 = positive_df_1950.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1950)\n",
    "negative_list_1950=negative_df_1950.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2753769552387656\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1950 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1950[ex]    # get its vector\n",
    "        vectorlength_1950 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1950.append(vec / vectorlength_1950)              # and save it in the list\n",
    "        \n",
    "    thesum_1950 = np.sum(vectors_1950, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1950 = np.linalg.norm(thesum_1950, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1950 / vectorlength_1950\n",
    "\n",
    "\n",
    "pos_vector_1950 = get_vector(positive_list_1950)\n",
    "neg_vector_1950 = get_vector(negative_list_1950)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1950 = cosine(vec_queer_1950, pos_vector_1950)\n",
    "neg_dist_1950 = cosine(vec_queer_1950, neg_vector_1950)\n",
    "\n",
    "    \n",
    "net_positive_queer_1950 = neg_dist_1950 - pos_dist_1950\n",
    "    \n",
    "print(net_positive_queer_1950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1960s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1960 = dict()\n",
    "\n",
    "hist_vocab_1960 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1960-vocab.pkl', 'rb'))\n",
    "hist_vectors_1960 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1960-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1960)):\n",
    "    hist_embeds_1960[hist_vocab_1960[i]] = hist_vectors_1960[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1960 = hist_embeds_1960['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1960 = dict()\n",
    "for v in hist_vocab_1960:\n",
    "    vectordict_1960[v] = hist_embeds_1960[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1960 = pd.DataFrame.from_dict(vectordict_1960, orient = 'index')\n",
    "vectordf_1960[\"sum\"] = vectordf_1960.sum(axis=1)\n",
    "valuezero_1960 = vectordf_1960['sum'].isin([0.0])\n",
    "vectordf_1960[~valuezero_1960]\n",
    "vectordf_no_zeros_1960 = vectordf_1960[~valuezero_1960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1960['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1960):\n",
    "    \n",
    "    vectordict_1960 = dict()\n",
    "    for v in hist_vocab_1960:\n",
    "        vectordict_1960[v] = hist_embeds_1960[v]\n",
    "        \n",
    "    vectordf_1960 = pd.DataFrame.from_dict(vectordict_1960, orient = 'index')\n",
    "    vectordf_1960[\"sum\"] = vectordf_1960.sum(axis=1)\n",
    "    valuezero_1960 = vectordf_1960['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1960 = vectordf_1960[~valuezero_1960]\n",
    "    del vectordf_no_zeros_1960['sum']\n",
    "    \n",
    "    vectorlengths_1960 = np.linalg.norm(vectordf_no_zeros_1960, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1960 = vectordf_no_zeros_1960.divide(vectorlengths_1960, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1960 = list(vectordf_no_zeros_1960.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1960 = pca.fit_transform(vectordf_no_zeros_1960)                      # perform PCA\n",
    "    plotframe_1960 = pd.DataFrame({'x': components_1960[ : , 0], 'y': components_1960[ : , 1]}, index = hist_vocab_no_zeros_1960)\n",
    "    \n",
    "    return plotframe_1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.01630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.14482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     queer\n",
       "x  0.01630\n",
       "y -0.14482"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1960 = vocab2pca(hist_vocab_1960)\n",
    "transposed_1960 = plotframe_1960.transpose()\n",
    "transposed_1960[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1960 = transposed_1960.iloc[0]['queer']\n",
    "queer_y_1960 = transposed_1960.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1960[\"distance_from_queer\"] = (((plotframe_1960.x-queer_x_1960)**2+(plotframe_1960.y-queer_y_1960)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1960 = plotframe_1960.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1960.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1960.index)[0:100]\n",
    "queersimilar_1960 = list(distance_sorted_1960.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1960 = [w for w in queersimilar_1960 if w in negative_words]\n",
    "positivelist_1960 = [w for w in queersimilar_1960 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1960  = vectordf_no_zeros_1960[vectordf_no_zeros_1960.index.isin(negativelist_1960)]\n",
    "positive_df_1960  = vectordf_no_zeros_1960[vectordf_no_zeros_1960.index.isin(positivelist_1960)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smiles', 'graceful', 'tenderly', 'fantastic', 'delicious', 'majestic', 'sweetly', 'comely', 'agility', 'joyfully']\n",
      "['cry', 'bore', 'weary', 'queer', 'sore', 'dismal', 'desperately', 'dreary', 'foul', 'coldly', 'nightmare', 'shrug', 'mock', 'fiend', 'commotion', 'uproar', 'crush', 'demon', 'lurking', 'drowning', 'chatter', 'monotonous', 'unfamiliar', 'choke', 'brood', 'boring', 'opponent', 'deformed', 'snare', 'mocked']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1960 = positive_df_1960.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1960)\n",
    "negative_list_1960 = negative_df_1960.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.21694947808635445\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1960 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1960[ex]    # get its vector\n",
    "        vectorlength_1960 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1960.append(vec / vectorlength_1960)              # and save it in the list\n",
    "        \n",
    "    thesum_1960 = np.sum(vectors_1960, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1960 = np.linalg.norm(thesum_1960, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1960 / vectorlength_1960\n",
    "\n",
    "\n",
    "pos_vector_1960 = get_vector(positive_list_1960)\n",
    "neg_vector_1960 = get_vector(negative_list_1960)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1960 = cosine(vec_queer_1960, pos_vector_1960)\n",
    "neg_dist_1960 = cosine(vec_queer_1960, neg_vector_1960)\n",
    "\n",
    "    \n",
    "net_positive_queer_1960 = neg_dist_1960 - pos_dist_1960\n",
    "    \n",
    "print(net_positive_queer_1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1970s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1970 = dict()\n",
    "\n",
    "hist_vocab_1970 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1970-vocab.pkl', 'rb'))\n",
    "hist_vectors_1970 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1970-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1970)):\n",
    "    hist_embeds_1970[hist_vocab_1970[i]] = hist_vectors_1970[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1970 = hist_embeds_1970['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1970 = dict()\n",
    "for v in hist_vocab_1970:\n",
    "    vectordict_1970[v] = hist_embeds_1970[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1970 = pd.DataFrame.from_dict(vectordict_1970, orient = 'index')\n",
    "vectordf_1970[\"sum\"] = vectordf_1970.sum(axis=1)\n",
    "valuezero_1970 = vectordf_1970['sum'].isin([0.0])\n",
    "vectordf_1970[~valuezero_1970]\n",
    "vectordf_no_zeros_1970 = vectordf_1970[~valuezero_1970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1970['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1970):\n",
    "    \n",
    "    vectordict_1970 = dict()\n",
    "    for v in hist_vocab_1970:\n",
    "        vectordict_1970[v] = hist_embeds_1970[v]\n",
    "        \n",
    "    vectordf_1970 = pd.DataFrame.from_dict(vectordict_1970, orient = 'index')\n",
    "    vectordf_1970[\"sum\"] = vectordf_1970.sum(axis=1)\n",
    "    valuezero_1970 = vectordf_1970['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1970 = vectordf_1970[~valuezero_1970]\n",
    "    del vectordf_no_zeros_1970['sum']\n",
    "    \n",
    "    vectorlengths_1970 = np.linalg.norm(vectordf_no_zeros_1970, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1970 = vectordf_no_zeros_1970.divide(vectorlengths_1970, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1970 = list(vectordf_no_zeros_1970.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1970 = pca.fit_transform(vectordf_no_zeros_1970)                      # perform PCA\n",
    "    plotframe_1970 = pd.DataFrame({'x': components_1970[ : , 0], 'y': components_1970[ : , 1]}, index = hist_vocab_no_zeros_1970)\n",
    "    \n",
    "    return plotframe_1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.031927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.140288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x  0.031927\n",
       "y -0.140288"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1970 = vocab2pca(hist_vocab_1970)\n",
    "transposed_1970 = plotframe_1970.transpose()\n",
    "transposed_1970[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1970 = transposed_1970.iloc[0]['queer']\n",
    "queer_y_1970 = transposed_1970.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1970[\"distance_from_queer\"] = (((plotframe_1970.x-queer_x_1970)**2+(plotframe_1970.y-queer_y_1970)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1970 = plotframe_1970.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1970.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1970.index)[0:100]\n",
    "queersimilar_1970 = list(distance_sorted_1970.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1970 = [w for w in queersimilar_1970 if w in negative_words]\n",
    "positivelist_1970 = [w for w in queersimilar_1970 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1970  = vectordf_no_zeros_1970[vectordf_no_zeros_1970.index.isin(negativelist_1970)]\n",
    "positive_df_1970  = vectordf_no_zeros_1970[vectordf_no_zeros_1970.index.isin(positivelist_1970)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['striking', 'eagerly', 'shine', 'tenderly', 'stately', 'orderly', 'sweetly', 'everlasting', 'intricate', 'cheery', 'agility']\n",
      "['shake', 'lonely', 'queer', 'sore', 'wounds', 'foul', 'depression', 'commotion', 'uproar', 'drowning', 'crush', 'sour', 'mysteriously', 'hysteria', 'gruff', 'indignantly', 'waning', 'maniac', 'scornfully', 'snare']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1970=positive_df_1970.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1970)\n",
    "negative_list_1970= negative_df_1970.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.19003084138205717\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1970 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1970[ex]    # get its vector\n",
    "        vectorlength_1970 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1970.append(vec / vectorlength_1970)              # and save it in the list\n",
    "        \n",
    "    thesum_1970 = np.sum(vectors_1970, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1970 = np.linalg.norm(thesum_1970, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1970 / vectorlength_1970\n",
    "\n",
    "\n",
    "pos_vector_1970 = get_vector(positive_list_1970)\n",
    "neg_vector_1970 = get_vector(negative_list_1970)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1970 = cosine(vec_queer_1970, pos_vector_1970)\n",
    "neg_dist_1970 = cosine(vec_queer_1970, neg_vector_1970)\n",
    "\n",
    "    \n",
    "net_positive_queer_1970 = neg_dist_1970 - pos_dist_1970\n",
    "    \n",
    "print(net_positive_queer_1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1980s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1980 = dict()\n",
    "\n",
    "hist_vocab_1980 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1980-vocab.pkl', 'rb'))\n",
    "hist_vectors_1980 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1980-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1980)):\n",
    "    hist_embeds_1980[hist_vocab_1980[i]] = hist_vectors_1980[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1980 = hist_embeds_1980['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1980 = dict()\n",
    "for v in hist_vocab_1980:\n",
    "    vectordict_1980[v] = hist_embeds_1980[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1980 = pd.DataFrame.from_dict(vectordict_1980, orient = 'index')\n",
    "vectordf_1980[\"sum\"] = vectordf_1980.sum(axis=1)\n",
    "valuezero_1980 = vectordf_1980['sum'].isin([0.0])\n",
    "vectordf_1980[~valuezero_1980]\n",
    "vectordf_no_zeros_1980 = vectordf_1980[~valuezero_1980]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1980['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1980):\n",
    "    \n",
    "    vectordict_1980 = dict()\n",
    "    for v in hist_vocab_1980:\n",
    "        vectordict_1980[v] = hist_embeds_1980[v]\n",
    "        \n",
    "    vectordf_1980 = pd.DataFrame.from_dict(vectordict_1980, orient = 'index')\n",
    "    vectordf_1980[\"sum\"] = vectordf_1980.sum(axis=1)\n",
    "    valuezero_1980 = vectordf_1980['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1980 = vectordf_1980[~valuezero_1980]\n",
    "    del vectordf_no_zeros_1980['sum']\n",
    "    \n",
    "    vectorlengths_1980 = np.linalg.norm(vectordf_no_zeros_1980, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1980 = vectordf_no_zeros_1980.divide(vectorlengths_1980, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1980 = list(vectordf_no_zeros_1980.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1980 = pca.fit_transform(vectordf_no_zeros_1980)                      # perform PCA\n",
    "    plotframe_1980 = pd.DataFrame({'x': components_1980[ : , 0], 'y': components_1980[ : , 1]}, index = hist_vocab_no_zeros_1980)\n",
    "    \n",
    "    return plotframe_1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>-0.042804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.163765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x -0.042804\n",
       "y -0.163765"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1980 = vocab2pca(hist_vocab_1980)\n",
    "transposed_1980 = plotframe_1980.transpose()\n",
    "transposed_1980[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1980 = transposed_1980.iloc[0]['queer']\n",
    "queer_y_1980 = transposed_1980.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotframe_1990[\"distance_from_queer\"] = (((plotframe_1980.x-queer_x_1980)**2+(plotframe_1980.y-queer_y_1980)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1980 = plotframe_1990.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1980.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1980.index)[0:100]\n",
    "queersimilar_1980 = list(distance_sorted_1980.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1980 = [w for w in queersimilar_1980 if w in negative_words]\n",
    "positivelist_1980 = [w for w in queersimilar_1980 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1980  = vectordf_no_zeros_1980[vectordf_no_zeros_1980.index.isin(negativelist_1980)]\n",
    "positive_df_1980  = vectordf_no_zeros_1980[vectordf_no_zeros_1980.index.isin(positivelist_1980)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strong', 'excitement', 'instantly', 'brilliant', 'striking', 'ecstasy', 'refreshed', 'jovial', 'tickle', 'blithe']\n",
      "['weak', 'confusion', 'strain', 'queer', 'harsh', 'haste', 'foul', 'oddly', 'depressed', 'lurking', 'isolated', 'frantic', 'hopelessly', 'involuntarily', 'impending', 'hysterical', 'giddy', 'aghast', 'infernal', 'mysteriously', 'opponent', 'enraged', 'unobserved', 'askance', 'drought', 'narrower', 'trample', 'mirage', 'fateful', 'guiltily']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1980=positive_df_1980.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1980)\n",
    "negative_list_1980=negative_df_1980.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.09638493868861475\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1980 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1980[ex]    # get its vector\n",
    "        vectorlength_1980 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1980.append(vec / vectorlength_1980)              # and save it in the list\n",
    "        \n",
    "    thesum_1980 = np.sum(vectors_1980, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1980 = np.linalg.norm(thesum_1980, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1980 / vectorlength_1980\n",
    "\n",
    "\n",
    "pos_vector_1980 = get_vector(positive_list_1980)\n",
    "neg_vector_1980 = get_vector(negative_list_1980)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1980 = cosine(vec_queer_1980, pos_vector_1980)\n",
    "neg_dist_1980 = cosine(vec_queer_1980, neg_vector_1980)\n",
    "\n",
    "    \n",
    "net_positive_queer_1980 = neg_dist_1980 - pos_dist_1980\n",
    "    \n",
    "print(net_positive_queer_1980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (1990s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_embeds_1990 = dict()\n",
    "\n",
    "hist_vocab_1990 = pickle.load(open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1990-vocab.pkl', 'rb'))\n",
    "hist_vectors_1990 = np.load('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\eng-fiction-all_sgns\\\\sgns\\\\1990-w.npy')\n",
    "\n",
    "\n",
    "for i in range(len(hist_vocab_1990)):\n",
    "    hist_embeds_1990[hist_vocab_1990[i]] = hist_vectors_1990[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_queer_1990 = hist_embeds_1990['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordict_1990 = dict()\n",
    "for v in hist_vocab_1990:\n",
    "    vectordict_1990[v] = hist_embeds_1990[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordf_1990 = pd.DataFrame.from_dict(vectordict_1990, orient = 'index')\n",
    "vectordf_1990[\"sum\"] = vectordf_1990.sum(axis=1)\n",
    "valuezero_1990 = vectordf_1990['sum'].isin([0.0])\n",
    "vectordf_1990[~valuezero_1990]\n",
    "vectordf_no_zeros_1990 = vectordf_1990[~valuezero_1990]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "del vectordf_no_zeros_1990['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab2pca(hist_vocab_1990):\n",
    "    \n",
    "    vectordict_1990 = dict()\n",
    "    for v in hist_vocab_1990:\n",
    "        vectordict_1990[v] = hist_embeds_1990[v]\n",
    "        \n",
    "    vectordf_1990 = pd.DataFrame.from_dict(vectordict_1990, orient = 'index')\n",
    "    vectordf_1990[\"sum\"] = vectordf_1990.sum(axis=1)\n",
    "    valuezero_1990 = vectordf_1990['sum'].isin([0.0])\n",
    "    vectordf_no_zeros_1990 = vectordf_1990[~valuezero_1990]\n",
    "    del vectordf_no_zeros_1990['sum']\n",
    "    \n",
    "    vectorlengths_1990 = np.linalg.norm(vectordf_no_zeros_1990, ord = 2, axis = 1)   # normalize vector lengths\n",
    "    vectordf_no_zeros_1990 = vectordf_no_zeros_1990.divide(vectorlengths_1990, axis = 'rows')      # for PCA\n",
    "    hist_vocab_no_zeros_1990 = list(vectordf_no_zeros_1990.index.values) # for index\n",
    "    \n",
    "    pca = PCA(n_components = 2)       \n",
    "    components_1990 = pca.fit_transform(vectordf_no_zeros_1990)                      # perform PCA\n",
    "    plotframe_1990 = pd.DataFrame({'x': components_1990[ : , 0], 'y': components_1990[ : , 1]}, index = hist_vocab_no_zeros_1990)\n",
    "    \n",
    "    return plotframe_1990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>queer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>-0.020595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.061811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      queer\n",
       "x -0.020595\n",
       "y -0.061811"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotframe_1990 = vocab2pca(hist_vocab_1990)\n",
    "transposed_1990 = plotframe_1990.transpose()\n",
    "transposed_1990[['queer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "queer_x_1990 = transposed_1990.iloc[0]['queer']\n",
    "queer_y_1990 = transposed_1990.iloc[1]['queer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotframe_1990[\"distance_from_queer\"] = (((plotframe_1990.x-queer_x_1990)**2+(plotframe_1990.y-queer_y_1990)**2)**0.5)\n",
    "pd.set_option('display.max_rows', None)\n",
    "distance_sorted_1990 = plotframe_1990.sort_values([\"distance_from_queer\"], ascending=True)\n",
    "distance_sorted_1990.head(100)\n",
    "pd.set_option('display.max_rows', None)\n",
    "list(distance_sorted_1990.index)[0:100]\n",
    "queersimilar_1990 = list(distance_sorted_1990.index)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liunegative.txt', encoding = 'utf-8') as f:\n",
    "    negative_words = [x.strip() for x in f.readlines()]\n",
    "    \n",
    "with open('C:\\\\Users\\\\Heejoung Shin\\\\Documents\\\\Heejoung Files\\\\UIUC MSLIS\\\\IS417-Data Science in the Humanities\\\\Research Paper\\\\Data\\\\liupositive.txt', encoding = 'utf-8') as f:\n",
    "    positive_words = [x.strip() for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativelist_1990 = [w for w in queersimilar_1990 if w in negative_words]\n",
    "positivelist_1990 = [w for w in queersimilar_1990 if w in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_df_1990  = vectordf_no_zeros_1990[vectordf_no_zeros_1990.index.isin(negativelist_1990)]\n",
    "positive_df_1990  = vectordf_no_zeros_1990[vectordf_no_zeros_1990.index.isin(positivelist_1990)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheer', 'warmly', 'terrific', 'passionately', 'quicker', 'triumphantly', 'quieter', 'stunning']\n",
      "['sadly', 'awkward', 'queer', 'curse', 'coldly', 'gravely', 'abrupt', 'wary', 'pinch', 'lame', 'unnoticed', 'gruff', 'slower', 'errant', 'cripple', 'forged', 'mirage', 'fateful', 'fussy', 'sardonically', 'pillory']\n"
     ]
    }
   ],
   "source": [
    "positive_list_1990=positive_df_1990.index.values.tolist() # save index values (the words) from positive_df into a list,\n",
    "                                                # because get_vector uses list (not dataframe, not dict) as input\n",
    "print(positive_list_1990)\n",
    "negative_list_1990=negative_df_1990.index.values.tolist() # save index values (the words) from negative_df into a list\n",
    "print(negative_list_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2506673331544481\n"
     ]
    }
   ],
   "source": [
    "def get_vector(wordlist): \n",
    "    vectors_1990 = [] # start with empty list\n",
    "    \n",
    "    for ex in wordlist:    # for each word in a wordlist\n",
    "        vec = vectordict_1990[ex]    # get its vector\n",
    "        vectorlength_1990 = np.linalg.norm(vec, ord = 2)     # normalize length\n",
    "        vectors_1990.append(vec / vectorlength_1990)              # and save it in the list\n",
    "        \n",
    "    thesum_1990 = np.sum(vectors_1990, axis = 0)                  # then add all the vectors\n",
    "    vectorlength_1990 = np.linalg.norm(thesum_1990, ord = 2)      # normalize length again\n",
    "    \n",
    "    return thesum_1990 / vectorlength_1990\n",
    "\n",
    "\n",
    "pos_vector_1990 = get_vector(positive_list_1990)\n",
    "neg_vector_1990 = get_vector(negative_list_1990)\n",
    "\n",
    "# vec_queer = model.wv['queer'] # vec_queer is calculated above\n",
    "    \n",
    "pos_dist_1990 = cosine(vec_queer_1990, pos_vector_1990)\n",
    "neg_dist_1990 = cosine(vec_queer_1990, neg_vector_1990)\n",
    "\n",
    "    \n",
    "net_positive_queer_1990 = neg_dist_1990 - pos_dist_1990\n",
    "    \n",
    "print(net_positive_queer_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x252 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "   \n",
    "decade = ['1850s', '1860s', '1870s', '1880s', '1890s', '1900s', '1910s', '1920s', '1930s', '1940s', '1950s', '1960s', '1970s', '1980s', '1990s']\n",
    "net_positivity = [-0.460357306235393,-0.072816471, -0.151096911, -0.197052511, -0.391922249, -0.21433233, -0.286137041\n",
    ", -0.206617386, -0.405334859, -0.338515494, -0.271758236, -0.216949478, -0.190030841, -0.096384939, -0.250667333]\n",
    "  \n",
    "plt.figure(figsize = (13, 3.5))\n",
    "plt.plot(decade, net_positivity, marker='o')\n",
    "plt.title(\"Associations around Queer from the 1850s to the 1990s in English Fiction\")\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Net Positivity')\n",
    "plt.grid(True)\n",
    "fig = plt.gcf()\n",
    "fig.savefig(\"netpositivity_histwords.png\", dpi = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
